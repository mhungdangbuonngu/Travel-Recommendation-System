{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install bs4\n",
        "!pip install datatime\n",
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNbOA88cNi_4",
        "outputId": "2909865e-f995-4caa-e533-dac7ce63630c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement datatime (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for datatime\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.23.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.7.4)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.7)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bp\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import time\n",
        "from selenium import webdriver\n",
        "\n",
        "\n",
        "USER_AGENT='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36 Edge/127.0.2651.105'\n",
        "REQUEST_HEADER = {\n",
        "    'User-Agent': USER_AGENT,\n",
        "    'Accept-language': 'en-US, en;q=0.5',\n",
        "}\n",
        "def get_page_html(url):\n",
        "    cleaned_url = url.strip().lstrip('\\ufeff')\n",
        "    res = requests.get(url=cleaned_url, headers=REQUEST_HEADER)\n",
        "    return res.text\n",
        "\n",
        "# def get_museum_price(soup):\n",
        "#     price_element=soup.find('div', attrs={'style':'color: rgb(255, 94, 31); font-size: 20px;'})\n",
        "#     if price_element:\n",
        "#         true_price = price_element.text.strip().replace('VND', '').replace('.', '')\n",
        "#         return float(true_price)\n",
        "#     return None\n",
        "\n",
        "def get_museum_name(soup):\n",
        "    name=soup.find('div',class_='iSVKr')\n",
        "    return name.text.strip() if name else None\n",
        "\n",
        "def get_museum_atttype(soup):\n",
        "    att_type=soup.find('div',class_='biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W KxBGd')\n",
        "    return att_type.text.strip() if att_type else None\n",
        "\n",
        "def get_museum_rating(soup):\n",
        "    rating=soup.find('div',class_ = 'biGQs _P fiohW hzzSG uuBRH')\n",
        "    return rating.text.strip() if rating else None\n",
        "\n",
        "# def get_museum_des(soup):\n",
        "#     des=soup.find('div',attrs={'style':'font-family:Godwit, -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Arial, sans-serif, Apple Color Emoji, Segoe UI Emoji, Segoe UI Symbol;font-size:14px;line-height:20px;max-height:80px;overflow:hidden'})\n",
        "#     return des.text.strip().replace('\\n','') if des else None\n",
        "\n",
        "def get_museum_duration(soup):\n",
        "    duration=soup.find('div',class_='nvXSy f _Y Q2')\n",
        "    return duration.text.strip().replace('Thời lượng: ', '').replace(' giờ', '') if duration else None\n",
        "\n",
        "def get_museum_location(soup):\n",
        "    location=soup.find('div',class_='MJ')\n",
        "    return location.text.strip().replace('Địa chỉ', '').replace(' 100000 Việt Nam', '') if location else None\n",
        "\n",
        "def get_museum_comments(soup):\n",
        "    comments=[]\n",
        "    a = soup.findAll('div',class_='_T FKffI bmUTE')\n",
        "    for comment in a:\n",
        "        comments.append(comment.text.strip().replace('Đọc thêm', ''))\n",
        "    return comments\n",
        "# def get_museum_img_url(soup):\n",
        "#     div=soup.find('div', class_='css-1dbjc4n r-j9b53g r-1i97xy8 r-1ta3fxp r-18u37iz r-1z0tv5g r-1udh08x')\n",
        "#     img_tag=div.findAll('img')\n",
        "#     img_url=[img['src'] for img in img_tag]\n",
        "\n",
        "#     return img_url\n",
        "\n",
        "def extract_museum_url(url,i):\n",
        "    info={}\n",
        "    print(f\"scraping URL number: {i}\")\n",
        "    html = get_page_html(url=url)\n",
        "    soup=bp(html,'lxml')\n",
        "    info['id']=i\n",
        "    info['name'] = get_museum_name(soup)\n",
        "    info['type']=get_museum_atttype(soup)\n",
        "    info['duration']=get_museum_duration(soup)\n",
        "    # info['price']=get_museum_price(soup)\n",
        "    info['rating']=get_museum_rating(soup)\n",
        "    info['location'] = get_museum_location(soup)\n",
        "    # info['description']=get_museum_des(soup)\n",
        "    info['comments']=get_museum_comments(soup)\n",
        "    # info['img_url']=get_museum_img_url(soup)\n",
        "    return info\n",
        "\n",
        "\n",
        "if __name__ ==\"__main__\":\n",
        "    data=[]\n",
        "    with open('/content/drive/MyDrive/GP2024Test/museums.csv',newline='',encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        i=0\n",
        "        for row in reader:\n",
        "            url=row[0]\n",
        "            file_name = \"/content/drive/MyDrive/GP2024Test/museum3.txt\"\n",
        "            data.append(extract_museum_url(url,i))\n",
        "            with open(file_name, 'w', encoding='utf-8') as file:\n",
        "                file.write(str(data))\n",
        "            i+=1\n",
        "            time.sleep(15)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k__LQZBcLQFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d08720-f574-423a-d421-e71df88ebe06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scraping URL number: 0\n",
            "scraping URL number: 1\n",
            "scraping URL number: 2\n",
            "scraping URL number: 3\n",
            "scraping URL number: 4\n",
            "scraping URL number: 5\n",
            "scraping URL number: 6\n",
            "scraping URL number: 7\n",
            "scraping URL number: 8\n",
            "scraping URL number: 9\n",
            "scraping URL number: 10\n",
            "scraping URL number: 11\n",
            "scraping URL number: 12\n",
            "scraping URL number: 13\n",
            "scraping URL number: 14\n",
            "scraping URL number: 15\n",
            "scraping URL number: 16\n",
            "scraping URL number: 17\n",
            "scraping URL number: 18\n",
            "scraping URL number: 19\n",
            "scraping URL number: 20\n",
            "scraping URL number: 21\n",
            "scraping URL number: 22\n",
            "scraping URL number: 23\n",
            "scraping URL number: 24\n",
            "scraping URL number: 25\n",
            "scraping URL number: 26\n",
            "scraping URL number: 27\n",
            "scraping URL number: 28\n",
            "scraping URL number: 29\n",
            "scraping URL number: 30\n",
            "scraping URL number: 31\n",
            "scraping URL number: 32\n",
            "scraping URL number: 33\n",
            "scraping URL number: 34\n",
            "scraping URL number: 35\n",
            "scraping URL number: 36\n",
            "scraping URL number: 37\n",
            "scraping URL number: 38\n",
            "scraping URL number: 39\n",
            "scraping URL number: 40\n",
            "scraping URL number: 41\n",
            "scraping URL number: 42\n",
            "scraping URL number: 43\n",
            "scraping URL number: 44\n",
            "scraping URL number: 45\n",
            "scraping URL number: 46\n",
            "scraping URL number: 47\n",
            "scraping URL number: 48\n",
            "scraping URL number: 49\n",
            "scraping URL number: 50\n",
            "scraping URL number: 51\n",
            "scraping URL number: 52\n",
            "scraping URL number: 53\n",
            "scraping URL number: 54\n",
            "scraping URL number: 55\n",
            "scraping URL number: 56\n",
            "scraping URL number: 57\n",
            "scraping URL number: 58\n",
            "scraping URL number: 59\n",
            "scraping URL number: 60\n",
            "scraping URL number: 61\n",
            "scraping URL number: 62\n",
            "scraping URL number: 63\n",
            "scraping URL number: 64\n",
            "scraping URL number: 65\n",
            "scraping URL number: 66\n",
            "scraping URL number: 67\n",
            "scraping URL number: 68\n",
            "scraping URL number: 69\n",
            "scraping URL number: 70\n",
            "scraping URL number: 71\n",
            "scraping URL number: 72\n",
            "scraping URL number: 73\n",
            "scraping URL number: 74\n",
            "scraping URL number: 75\n",
            "scraping URL number: 76\n",
            "scraping URL number: 77\n",
            "scraping URL number: 78\n",
            "scraping URL number: 79\n",
            "scraping URL number: 80\n",
            "scraping URL number: 81\n",
            "scraping URL number: 82\n",
            "scraping URL number: 83\n",
            "scraping URL number: 84\n",
            "scraping URL number: 85\n",
            "scraping URL number: 86\n",
            "scraping URL number: 87\n",
            "scraping URL number: 88\n",
            "scraping URL number: 89\n",
            "scraping URL number: 90\n",
            "scraping URL number: 91\n",
            "scraping URL number: 92\n",
            "scraping URL number: 93\n",
            "scraping URL number: 94\n",
            "scraping URL number: 95\n",
            "scraping URL number: 96\n",
            "scraping URL number: 97\n"
          ]
        }
      ]
    }
  ]
}